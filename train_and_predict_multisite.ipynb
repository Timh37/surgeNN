{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72e934e1-daa2-4f72-bdfd-7f05d0cf6c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-10 08:24:21.742720: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-10 08:24:21.824237: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import xarray as xr\n",
    "import fnmatch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import keras\n",
    "from surgeNN import io, preprocessing\n",
    "from surgeNN.io import train_predict_output_to_ds, setup_output_dirs, add_loss_to_output\n",
    "from surgeNN.denseLoss import get_denseloss_weights #if starting with a clean environment, first, in terminal, do->'mamba install kdepy'\n",
    "from surgeNN.evaluation import add_error_metrics_to_prediction_ds\n",
    "from surgeNN.models import build_LSTM_stacked,build_LSTM_stacked_multioutput_static\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "import random\n",
    "from functools import reduce\n",
    "import gcsfs\n",
    "fs = gcsfs.GCSFileSystem() #list stores, stripp zarr from filename, load \n",
    "\n",
    "import gc #callback to clean up garbage after each epoch, not sure if strictly necessary (usage: callbacks = [GC_Callback()])\n",
    "class GC_Callback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36e2c643-618d-4ce7-a790-69da45f672d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#settings\n",
    "tgs        = ['stavanger-svg-nor-nhs.csv','wick-wic-gbr-bodc.csv','esbjerg-esb-dnk-dmi.csv',\n",
    "              'immingham-imm-gbr-bodc.csv','den_helder-denhdr-nld-rws.csv', 'fishguard-fis-gbr-bodc.csv',  \n",
    "              'brest-822a-fra-uhslc.csv', 'vigo-vigo-esp-ieo.csv',  'alicante_i_outer_harbour-alio-esp-da_mm.csv'] #all tide gauges to process\n",
    "tgs = ['den_helder-denhdr-nld-rws.csv','brest-822a-fra-uhslc.csv']\n",
    "region_name = 'test'\n",
    "model_architecture = 'lstm'\n",
    "predictor_degrees = 5\n",
    "\n",
    "#i/o\n",
    "predictor_path  = 'gs://leap-persistent/timh37/era5_predictors/3hourly'\n",
    "predictand_path = '/home/jovyan/test_surge_models/input/t_tide_3h_hourly_deseasoned_predictands'\n",
    "splits = 'chronological' #'stratified'\n",
    "output_dir = os.path.join('/home/jovyan/test_surge_models/results/nns_highresmip/gesla3',splits) #'/home/jovyan/test_surge_models/results/nns/'\n",
    "store_model = 0#1 #whether to store the tensorflow models\n",
    "temp_freq = 3 # [hours] temporal frequency to use\n",
    "\n",
    "#training\n",
    "predictor_vars = ['msl','u10','v10'] #variables to use\n",
    "n_runs = 1 #how many hyperparameter combinations to run\n",
    "n_iterations = 1 #how many iterations to run per hyperparameter combination\n",
    "n_epochs = 1 #how many training epochs\n",
    "patience = 10 #early stopping patience\n",
    "loss_function = {'mse':'mse'} # default tensorflow loss function string or string of custom loss function of surgeNN.losses (e.g., 'gevl({gamma})')\n",
    "\n",
    "#splitting & stratified sampling\n",
    "split_fractions = [.6,.2,.2] #train, test, val\n",
    "strat_metric = '99pct'\n",
    "strat_start_month = 7\n",
    "strat_seed = 0\n",
    "\n",
    "#hyperparameters:\n",
    "\n",
    "dl_alpha = np.array([0,3]).astype('int') #defined from command line\n",
    "batch_size = np.array([128]).astype('int')\n",
    "n_steps = np.array([9]).astype('int')\n",
    "n_convlstm = np.array([1]).astype('int')\n",
    "n_convlstm_units = np.array([32]).astype('int')\n",
    "n_dense = np.array([2]).astype('int')\n",
    "n_dense_units = np.array([32]).astype('int')\n",
    "dropout = np.array([0.2])#np.array([0.1,0.2])\n",
    "lrs = np.array([5e-5])#np.array([1e-5,5e-5,1e-4])\n",
    "l1s = np.array([0.02])\n",
    "\n",
    "hyperparam_options = [batch_size, n_steps, n_convlstm, n_convlstm_units,\n",
    "                n_dense, n_dense_units, dropout, lrs, l1s, dl_alpha]\n",
    "\n",
    "n_static=2 #test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c37bcf32-015c-4a64-9416-643b129be606",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/xarray/core/common.py:613: FutureWarning: Updating MultiIndexed coordinate 'c' would corrupt indices for other variables: ['latitude', 'longitude']. This will raise an error in the future. Use `.drop_vars({'latitude', 'c', 'longitude'})` before assigning new coordinate values.\n",
      "  data.coords.update(results)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "526/526 - 8s - loss: 4.6793 - dense_6_loss: 2.2865 - dense_7_loss: 2.1680 - val_loss: 1.8614 - val_dense_6_loss: 1.0736 - val_dense_7_loss: 0.5186 - 8s/epoch - 15ms/step\n",
      "Epoch 2/2\n",
      "526/526 - 6s - loss: 2.9247 - dense_6_loss: 1.3792 - dense_7_loss: 1.2451 - val_loss: 1.4564 - val_dense_6_loss: 0.7685 - val_dense_7_loss: 0.4148 - 6s/epoch - 12ms/step\n"
     ]
    }
   ],
   "source": [
    "setup_output_dirs(output_dir,store_model,model_architecture)\n",
    "\n",
    "lf_name = list(loss_function.keys())[0]\n",
    "lf = list(loss_function.values())[0]\n",
    "\n",
    "try:\n",
    "    lf = eval(lf)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "n_cells = int(predictor_degrees * (4/1))\n",
    "\n",
    "### (1) Load & prepare input data\n",
    "predictors = []\n",
    "for tg in tgs:\n",
    "    predictor = io.Predictor(predictor_path)\n",
    "    predictor.open_dataset(tg,predictor_vars,n_cells)\n",
    "    predictors.append(predictor.data.swap_dims({'lat_around_tg':'latitude','lon_around_tg':'longitude'}).stack(c=('latitude','longitude')))\n",
    "\n",
    "merged_predictors = xr.concat(predictors,dim='c').drop_duplicates(dim='c')\n",
    "merged_predictors = merged_predictors.assign_coords({'c':np.arange(len(merged_predictors.c))})\n",
    "\n",
    "predictors = io.Predictor(predictor_path)\n",
    "predictors.data = merged_predictors\n",
    "predictors.trim_years(1979,2017)\n",
    "predictors.subtract_annual_means()\n",
    "predictors.deseasonalize()\n",
    "\n",
    "predictands = []\n",
    "for t,tg in enumerate(tgs):\n",
    "    \n",
    "    predictand = io.Predictand(predictand_path)\n",
    "    predictand.open_dataset(tg)\n",
    "    predictand.trim_dates(predictors.data.time.isel(time=0).values,predictors.data.time.isel(time=-1).values)\n",
    "    predictand.deseasonalize()\n",
    "    predictand.resample_fillna(str(temp_freq)+'h')\n",
    "    \n",
    "    if t==0:\n",
    "        predictand0 = predictand\n",
    "        \n",
    "    predictand.data = predictand.data.rename(columns={\"surge\":\"surge_\"+str(t), \"lon\": \"lon_\"+str(t),\"lat\":\"lat_\"+str(t)})\n",
    "    predictands.append(predictand.data)\n",
    "\n",
    "predictands_merged = reduce(lambda  left,right: pd.merge(left,right,on=['date'],how='inner'), predictands) #merge predictands for the sites\n",
    "\n",
    "for k in np.arange(len(tgs)):\n",
    "    predictands_merged.loc[predictands_merged.isnull().any(axis=1), 'surge_'+str(k)] = np.nan #set complete rows to np.nan where at least one tg (column) has no observations\n",
    "\n",
    "predictand0.data['surge'] = predictands_merged['surge_0']\n",
    "\n",
    "### (2) Configure sets of hyperparameters to run with\n",
    "all_settings = list(itertools.product(*hyperparam_options))\n",
    "n_settings = len(all_settings)\n",
    "\n",
    "if n_runs<n_settings:\n",
    "    selected_settings = random.sample(all_settings, n_runs)\n",
    "else:\n",
    "    selected_settings = all_settings\n",
    "\n",
    "    \n",
    "### (3) Execute training & evaluation (n_iterations * n_runs times):\n",
    "for it in np.arange(n_iterations): #for each iteration\n",
    "    tg_datasets = [] #list to store output\n",
    "\n",
    "    for i,these_settings in enumerate(selected_settings): #for each set of hyperparameters\n",
    "\n",
    "        this_batch_size,this_n_steps,this_n_convlstm,this_n_convlstm_units,this_n_dense,this_n_dense_units,this_dropout,this_lr,this_l2,this_dl_alpha = these_settings #pick hyperparameters for this run\n",
    "       \n",
    "        #generate train, validation and test splits (fow now chronological, to-do: think about stratification for multiple sites\n",
    "        #predictand0 = predictands_merged[['surge_0','date']].rename(columns={\"surge_0\":\"surge\"}) #quickfix to make this work with current preprocessing scripts\n",
    "        \n",
    "        model_input = preprocessing.Input(predictors,predictand0) #use predictand for tg0 to do the splitting --> not ideal, other tgs may have a different distribution\n",
    "        model_input.stack_predictor_coords()\n",
    "        \n",
    "        if splits=='stratified':\n",
    "            model_input.split_stratified(split_fractions,this_n_steps,strat_start_month,strat_seed,strat_metric)\n",
    "        elif splits=='chronological':\n",
    "            model_input.split_chronological(split_fractions,this_n_steps)\n",
    "        \n",
    "        #index all predictands using the splitting indices\n",
    "        [y_train_,y_val_,y_test_] = [predictands_merged.iloc[k].filter(like='surge').values for k in [model_input.idx_train,model_input.idx_val,model_input.idx_test]] #set first few timesteps to np.nan? to-do?\n",
    "        y_train_[:,0] = model_input.y_train\n",
    "        y_val_[:,0] = model_input.y_val\n",
    "        y_test_[:,0] = model_input.y_test\n",
    "        \n",
    "        y_train_[np.any(np.isnan(y_train_),axis=1),:] = np.nan\n",
    "        y_val_[np.any(np.isnan(y_val_),axis=1),:] = np.nan\n",
    "        y_test_[np.any(np.isnan(y_test_),axis=1),:] = np.nan\n",
    "        \n",
    "        model_input.y_train = y_train_\n",
    "        model_input.y_val = y_val_\n",
    "        model_input.y_test = y_test_\n",
    "        \n",
    "        y_train_mean,y_train_sd = model_input.standardize()\n",
    "        model_input.compute_denseloss_weights(this_dl_alpha)\n",
    "        \n",
    "        x_train,y_train,w_train = model_input.get_windowed_filtered_np_input('train',this_n_steps) #generate input for neural network model\n",
    "        x_val,y_val,w_val       = model_input.get_windowed_filtered_np_input('val',this_n_steps)\n",
    "        x_test,y_test,w_test    = model_input.get_windowed_filtered_np_input('test',this_n_steps)\n",
    "                \n",
    "     \n",
    "        o_train,o_val,o_test = [y_train_sd * k + y_train_mean for k in [y_train,y_val,y_test]] #back-transform observations\n",
    "        \n",
    "        model = build_LSTM_stacked(this_n_convlstm, this_n_dense,(np.ones(this_n_convlstm)*this_n_convlstm_units).astype(int), \n",
    "                                  (np.ones(this_n_dense)*this_n_dense_units).astype(int),(this_n_steps,len(predictors.data.c) * len(predictor_vars)),\n",
    "                                                   len(tgs),'lstm0',this_dropout, this_lr, lf,l2=this_l2)\n",
    "       \n",
    "        train_history = model.fit(x=x_train,y=list(np.transpose(y_train)),epochs=n_epochs,batch_size=this_batch_size,\n",
    "                                  sample_weight=list(np.transpose(w_train)),validation_data=(x_val,list(np.transpose(y_val)),list(np.transpose(w_val))),\n",
    "                                      callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience,restore_best_weights=True),GC_Callback()],verbose=2) #with numpy arrays input\n",
    "        \n",
    "        #make predictions & back-transform\n",
    "        yhat_train = np.hstack(model.predict(x_train,verbose=0))*y_train_sd + y_train_mean\n",
    "        yhat_val = np.hstack(model.predict(x_val,verbose=0))*y_train_sd + y_train_mean\n",
    "        yhat_test = np.hstack(model.predict(x_test,verbose=0))*y_train_sd + y_train_mean\n",
    "        \n",
    "       \n",
    "        ''' #to be further developed\n",
    "        elif model_architecture == 'lstm_static':\n",
    "            \n",
    "            train_conditions = [np.transpose(np.tile(np.array([predictands[k].iloc[0,2],predictands[k].iloc[0,3]])[::,np.newaxis],y_train.shape[0])) for k in np.arange(len(tgs))]\n",
    "            val_conditions = [np.transpose(np.tile(np.array([predictands[k].iloc[0,2],predictands[k].iloc[0,3]])[::,np.newaxis],y_val.shape[0])) for k in np.arange(len(tgs))]\n",
    "            test_conditions = [np.transpose(np.tile(np.array([predictands[k].iloc[0,2],predictands[k].iloc[0,3]])[::,np.newaxis],y_test.shape[0])) for k in np.arange(len(tgs))]\n",
    "            \n",
    "            model = build_LSTM_stacked_multioutput_static(this_n_convlstm, this_n_dense,(np.ones(this_n_convlstm)*this_n_convlstm_units).astype(int), \n",
    "                                  (np.ones(this_n_dense)*this_n_dense_units).astype(int),(this_n_steps,np.prod([predictors.dims[k] for k in predictors.dims if k!='time']) * len(predictor_vars)),\n",
    "                                                   len(tgs),train_conditions[0].shape[-1],'lstm0',this_dropout, this_lr, lf,l2=this_l2)\n",
    "\n",
    "            train_history = model.fit(x=[x_train]+train_conditions,y=list(np.transpose(y_train)),epochs=n_epochs,batch_size=this_batch_size,sample_weight=list(np.transpose(w_train)),validation_data=([x_val]+val_conditions,list(np.transpose(y_val)),list(np.transpose(w_val))),\n",
    "                                          callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience,restore_best_weights=True),GC_Callback()],verbose=2) #with numpy arrays input\n",
    "            \n",
    "            yhat_train = np.hstack(model.predict([x_train]+train_conditions,verbose=0))*y_train_sd + y_train_mean\n",
    "            yhat_val = np.hstack(model.predict([x_val]+val_conditions,verbose=0))*y_train_sd + y_train_mean\n",
    "            yhat_test = np.hstack(model.predict([x_test]+test_conditions,verbose=0))*y_train_sd + y_train_mean\n",
    "        \n",
    "        #test multi-output model\n",
    "        '''\n",
    "        \n",
    "\n",
    "        \n",
    "        #store results into xr dataset for current settings and iteration\n",
    "        ds_train = train_predict_output_to_ds(o_train,yhat_train,model_input.t_train,these_settings,tgs,model_architecture,lf_name)\n",
    "        ds_val = train_predict_output_to_ds(o_val,yhat_val,model_input.t_val,these_settings,tgs,model_architecture,lf_name)\n",
    "        ds_test = train_predict_output_to_ds(o_test,yhat_test,model_input.t_test,these_settings,tgs,model_architecture,lf_name)\n",
    "        \n",
    "        ds_i = xr.concat((ds_train,ds_val,ds_test),dim='split',coords='different') #concatenate results for each split\n",
    "        ds_i = ds_i.assign_coords(split = ['train','val','test'])\n",
    "\n",
    "        ds_i = add_loss_to_output(ds_i,train_history,n_epochs)\n",
    "        tg_datasets.append(ds_i) #append output of current iteration to list of all outputs\n",
    "        \n",
    "        if store_model:\n",
    "            my_path = os.path.join(output_dir,'keras_models',model_architecture)\n",
    "            my_fn = model_architecture+'_'+str(temp_freq)+'h_'+region_name+'_'+lf_name+'_hp1_i'+str(i)+'_it'\n",
    "\n",
    "            model.save(os.path.join(my_path,\n",
    "             my_fn+str(len(fnmatch.filter(os.listdir(my_path),my_fn+'*')))+'.keras'))\n",
    "\n",
    "        del model, train_history, ds_i #, x_train, x_val, x_test\n",
    "        tf.keras.backend.clear_session()\n",
    "        gc.collect()\n",
    "        \n",
    "    #concatenate across runs & compute statistics\n",
    "    out_ds = xr.concat(tg_datasets,dim='i',coords='different')\n",
    "    out_ds = add_error_metrics_to_prediction_ds(out_ds,[.95,.98,.99,.995],3) #optional third argument 'max_numT_between_isolated_extremes' to exclude extremes isolated by more than n timesteps from another extreme from evaluation (to avoid including extremes mainly due to semi-diurnal tides, see manuscript for more explanation)\n",
    "\n",
    "    out_ds = out_ds.assign_coords(lon = ('tg',np.array(np.array(predictands_merged.filter(like='lon').iloc[0]))))\n",
    "    out_ds = out_ds.assign_coords(lat = ('tg',np.array(np.array(predictands_merged.filter(like='lat').iloc[0]))))\n",
    "\n",
    "    if len(n_steps) == 1: #if n_steps is constant across i, obs doesn't need to have i as a dimension. Saves storage.\n",
    "        out_ds['o'] = out_ds['o'].isel(i=0,drop=True)\n",
    "\n",
    "    out_ds.attrs['temp_freq'] = temp_freq\n",
    "    out_ds.attrs['n_cells'] = n_cells\n",
    "    out_ds.attrs['n_epochs'] = n_epochs\n",
    "    out_ds.attrs['patience'] = patience\n",
    "    out_ds.attrs['loss_function'] = lf_name\n",
    "    out_ds.attrs['split_fractions'] = split_fractions\n",
    "    out_ds.attrs['stratification'] = strat_metric+'_'+str(strat_start_month)+'_'+str(strat_seed)\n",
    "\n",
    "    my_path = os.path.join(output_dir,'performance',model_architecture)\n",
    "    my_fn = model_architecture+'_'+str(temp_freq)+'h_'+region_name+'_'+lf_name+'_hp1_ndeg'+str(predictor_degrees)+'_it'\n",
    "    #my_fn = model_architecture+'_'+str(temp_freq)+'h_'+tg.replace('.csv','')+'_'+lf_name+'_hp1_ndeg'+str(predictor_degrees)+'_it'\n",
    "    \n",
    "    #out_ds.to_netcdf(os.path.join(my_path,my_fn+str(len(fnmatch.filter(os.listdir(my_path),my_fn+'*')))+'.nc'),mode='w')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb3c86d-c7b3-4893-acc7-d95873c0f1e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
