{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39f6db52-8855-4fee-b7dc-a1820c5fa819",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20521/165059406.py:10: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n",
      "2024-10-18 13:20:44.722942: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-18 13:20:44.829643: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "'''script to regrid CMIP6 datatsets to target grid and store them'''\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import dask\n",
    "import os\n",
    "import intake\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from collections import defaultdict\n",
    "from tqdm.autonotebook import tqdm\n",
    "from xmip.utils import google_cmip_col\n",
    "from xmip.postprocessing import combine_datasets,_concat_sorted_time, merge_variables\n",
    "from cmip_catalogue_operations import reduce_cat_to_max_num_realizations, drop_vars_from_cat, drop_older_versions\n",
    "from cmip_ds_dict_operations import select_period, pr_flux_to_m, drop_duplicate_timesteps, drop_coords, drop_incomplete\n",
    "import xesmf as xe\n",
    "import gcsfs\n",
    "import keras\n",
    "from open_era5_predictors import get_era5_around_tgs\n",
    "from surgeNN.preprocessing import deseasonalize_da\n",
    "from surgeNN.io import load_predictors\n",
    "fs = gcsfs.GCSFileSystem() #list stores, stripp zarr from filename, load \n",
    "\n",
    "def generate_windowed_filtered_np_input(x,y,n_steps,w=None):\n",
    "    '''\n",
    "    Generate numpy arrays of windowed nan-filtered input data\n",
    "    Input:\n",
    "        x: predictors\n",
    "        y: predictands\n",
    "        n_steps: number of timesteps to use predictors at\n",
    "        w: sample weights of predictands, optional\n",
    "    Output:\n",
    "        x_out: windowed, nan-filtered predictors\n",
    "        y_out: nan-filtered predictands\n",
    "    '''\n",
    "    x_out = np.stack([x[k:k+n_steps,:] for k in np.arange(x.shape[0])][0:-(n_steps-1)],axis=0) #create windowed predictor array (x(t=-n_steps to t=0) to predict y(t=0)\n",
    "    \n",
    "    #filter where y is nan\n",
    "    where_y_is_finite = np.isfinite(y)\n",
    "    x_out = x_out[where_y_is_finite,...]\n",
    "    y_out = y[where_y_is_finite]\n",
    "\n",
    "    if w is not None: #do the same for the weights, if any\n",
    "        w_out = w[where_y_is_finite]\n",
    "        return x_out,y_out,w_out\n",
    "    else:\n",
    "        return x_out,y_out\n",
    "        \n",
    "def stack_predictors_for_lstm(predictors,var_names):\n",
    "    ''' stack predictors to prepare for lstm input'''\n",
    "    return np.reshape(np.stack([predictors[k].values for k in var_names],axis=-1),\n",
    "                      (len(predictors.time),len(predictors.latitude) * len(predictors.longitude) * len(var_names))) #stack grid cells & variables\n",
    "\n",
    "def stack_predictors_for_convlstm(predictors,var_names):\n",
    "    ''' stack predictors to prepare for convlstm input'''\n",
    "    return np.stack([predictors[k].values for k in var_names],axis=-1) #stack variables\n",
    "\n",
    "\n",
    "def deseasonalize_da(da):\n",
    "    '''subtract long-term monthly means from variable in dataset'''\n",
    "    \n",
    "    deseasoned_da = da.groupby(da.time.dt.month) - da.groupby(da.time.dt.month).mean('time')\n",
    "    \n",
    "    deseasoned_da = deseasoned_da + (da.mean(dim='time') - deseasoned_da.mean(dim='time'))\n",
    "\n",
    "    return deseasoned_da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5eec073-bbab-4c4a-b81a-c66bcea5b41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "highresmip_model = 'HadGEM3-GC31-HM'\n",
    "predictor_dir = 'gs://leap-persistent/timh37/HighResMIP/surgeNN_predictors/'\n",
    "nn_model_dir = '/home/jovyan/test_surge_models/results/nn_tests/keras_models/lstm'\n",
    "\n",
    "output_dir = '/home/jovyan/test_surge_models/results/nn_tests/highresmip_predictions'\n",
    "var_names = ['msl','u10','v10','w']\n",
    "n_steps = 9\n",
    "\n",
    "only_alpha0 = False\n",
    "standardize_predictors_with_era5 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fd6fbc3-ddd7-4e4c-9fac-202d68b1bec7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['leap-persistent/timh37/HighResMIP/surgeNN_predictors/predictors_HadGEM3-GC31-HM_1950_2050_alicante_i_outer_harbour-alio-esp-da_mm.zarr',\n",
       " 'leap-persistent/timh37/HighResMIP/surgeNN_predictors/predictors_HadGEM3-GC31-HM_1950_2050_brest-822a-fra-uhslc.zarr',\n",
       " 'leap-persistent/timh37/HighResMIP/surgeNN_predictors/predictors_HadGEM3-GC31-HM_1950_2050_den_helder-denhdr-nld-rws.zarr',\n",
       " 'leap-persistent/timh37/HighResMIP/surgeNN_predictors/predictors_HadGEM3-GC31-HM_1950_2050_esbjerg-esb-dnk-dmi.zarr',\n",
       " 'leap-persistent/timh37/HighResMIP/surgeNN_predictors/predictors_HadGEM3-GC31-HM_1950_2050_fishguard-fis-gbr-bodc.zarr',\n",
       " 'leap-persistent/timh37/HighResMIP/surgeNN_predictors/predictors_HadGEM3-GC31-HM_1950_2050_immingham-imm-gbr-bodc.zarr',\n",
       " 'leap-persistent/timh37/HighResMIP/surgeNN_predictors/predictors_HadGEM3-GC31-HM_1950_2050_stavanger-svg-nor-nhs.zarr',\n",
       " 'leap-persistent/timh37/HighResMIP/surgeNN_predictors/predictors_HadGEM3-GC31-HM_1950_2050_vigo-vigo-esp-ieo.zarr',\n",
       " 'leap-persistent/timh37/HighResMIP/surgeNN_predictors/predictors_HadGEM3-GC31-HM_1950_2050_wick-wic-gbr-bodc.zarr']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fs.ls('gs://leap-persistent/timh37/HighResMIP/surgeNN_predictors/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a31b2746-52df-4073-8265-57396adb5ab6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing: stavanger-svg-nor-nhs.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-18 13:21:11.232410: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/nvidia/lib64\n",
      "2024-10-18 13:21:11.232436: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9090/9090 [==============================] - 20s 2ms/step\n",
      "processing: wick-wic-gbr-bodc.csv\n",
      "9090/9090 [==============================] - 21s 2ms/step\n",
      "processing: esbjerg-esb-dnk-dmi.csv\n",
      "9090/9090 [==============================] - 20s 2ms/step\n",
      "processing: immingham-imm-gbr-bodc.csv\n",
      "9090/9090 [==============================] - 20s 2ms/step\n",
      "processing: den_helder-denhdr-nld-rws.csv\n",
      "9090/9090 [==============================] - 21s 2ms/step\n",
      "processing: fishguard-fis-gbr-bodc.csv\n",
      "9090/9090 [==============================] - 21s 2ms/step\n",
      "processing: brest-822a-fra-uhslc.csv\n",
      "9090/9090 [==============================] - 20s 2ms/step\n",
      "processing: vigo-vigo-esp-ieo.csv\n",
      "9090/9090 [==============================] - 20s 2ms/step\n",
      "processing: alicante_i_outer_harbour-alio-esp-da_mm.csv\n",
      "9090/9090 [==============================] - 20s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "surge_output = []\n",
    "for output_tg in ['stavanger-svg-nor-nhs.csv','wick-wic-gbr-bodc.csv','esbjerg-esb-dnk-dmi.csv',\n",
    "                  'immingham-imm-gbr-bodc.csv','den_helder-denhdr-nld-rws.csv', 'fishguard-fis-gbr-bodc.csv',  \n",
    "                  'brest-822a-fra-uhslc.csv', 'vigo-vigo-esp-ieo.csv',  'alicante_i_outer_harbour-alio-esp-da_mm.csv']:#tg_coords.tg.values:\n",
    "    print('processing: '+output_tg)\n",
    "    output_tg = output_tg.replace('.csv','')\n",
    "    \n",
    "    predictors = xr.open_dataset(os.path.join(predictor_dir,'predictors_'+highresmip_model+'_1950_2050_'+output_tg+'.zarr'),engine='zarr')\n",
    "\n",
    "    #find best lstm and load it\n",
    "    lstm = xr.open_mfdataset('/home/jovyan/test_surge_models/results/nn_tests/performance/lstm/lstm_3h*'+output_tg+'_mse_hp1_it*',combine='nested',concat_dim='it').load()\n",
    "    \n",
    "    if only_alpha0:\n",
    "        lstm = lstm.where(lstm.hyperparameters.sel(p='dl_alpha') == 0)\n",
    "        \n",
    "    idxmin = lstm.rmse_extremes.sel(split='val').sel(quantile=.99).argmin(dim=['i','it']) #lowest rmse extremes, could also use f1?\n",
    "    \n",
    "    ###### derive original x_train_mean and sd (not sure how best to standardize CMIP predictors)\n",
    "    era5_predictors = load_predictors('gs://leap-persistent/timh37/era5_predictors/'+'3hourly',output_tg+'.csv',5) \n",
    "    era5_predictors = era5_predictors.sel(time=slice('1979','2017')) #2018 because of end year GTSM simulations that are used as benchmark\n",
    "\n",
    "    if 'w' in var_names and 'w' not in era5_predictors.variables:\n",
    "        era5_predictors['w'] == np.sqrt((era5_predictors.u10**2+era5_predictors.v10**2))\n",
    "\n",
    "    for var in var_names: #remove amean\n",
    "        era5_predictors[var] = era5_predictors[var].groupby(era5_predictors.time.dt.year) - era5_predictors[var].groupby(era5_predictors.time.dt.year).mean('time') #remove annual means\n",
    "        era5_predictors[var] = deseasonalize_da(era5_predictors[var]) #remove mean seasonal cycle\n",
    "    \n",
    "    x_train = era5_predictors.sel(time=lstm.time.where(lstm.o.isel(i=idxmin['i'].values,it=idxmin['it'].values).sel(split='train')))\n",
    "    \n",
    "    x_train_mean = x_train.mean(dim='time') #skips nan by default\n",
    "    x_train_sd = x_train.std(dim='time',ddof=0) #skips nan by default\n",
    "    ###### \n",
    "\n",
    "    #derive backtransform\n",
    "    y_train_mean = np.nanmean(lstm.isel(i=idxmin['i'].values,it=idxmin['it'].values).o.sel(split='train'))\n",
    "    y_train_sd = np.nanstd(lstm.isel(i=idxmin['i'].values,it=idxmin['it'].values).o.sel(split='train'),ddof=0)\n",
    "    \n",
    "    model = keras.models.load_model(os.path.join(nn_model_dir,'lstm_3h_'+output_tg+'_mse_hp1_i'+str(idxmin['i'].values)+'_it'+str(idxmin['it'].values)+'.keras'))\n",
    "    \n",
    "    #preprocess predictors:\n",
    "    if 'w' in var_names and 'w' not in predictors.variables:\n",
    "        predictors['w'] = np.sqrt((predictors.u10**2+predictors.v10**2))\n",
    "            \n",
    "    for var in var_names: #remove amean\n",
    "        predictors[var] = predictors[var].groupby(predictors.time.dt.year) - predictors[var].groupby(predictors.time.dt.year).mean('time') #remove annual means\n",
    "        predictors[var] = deseasonalize_da(predictors[var]) #remove mean seasonal cycle\n",
    "    \n",
    "    if standardize_predictors_with_era5:\n",
    "        predictors = (predictors - x_train_mean)/x_train_sd\n",
    "    else:\n",
    "        predictors = (predictors - predictors.mean(dim='time'))/predictors.std(dim='time',ddof=0) #standardize\n",
    "\n",
    "    if model_type.lower()=='mlr':\n",
    "        predictors['stacked'] = predictors[var_names].to_array(dim=\"var\") #put predictor variables into one array\n",
    "        predictors = predictors[['stacked']]\n",
    "        predictors['stacked'] = predictors['stacked'].transpose(\"time\",\"var\",\"lon_around_tg\",...)#.stack(f=['var','lon_around_tg','lat_around_tg'],create_index=False)\n",
    "    \n",
    "    \n",
    "    x = stack_predictors_for_lstm(predictors,var_names) #put into right format\n",
    "    x,y = generate_windowed_filtered_np_input(x,np.zeros(x.shape[0]-n_steps+1),n_steps)\n",
    "    y_out = model.predict(x)\n",
    "    yhat = y_out*y_train_sd + y_train_mean\n",
    "    \n",
    "    surge_output.append(\n",
    "        \n",
    "        xr.Dataset(\n",
    "    data_vars=dict(\n",
    "        surge=([\"time\",\"tg\"], yhat),\n",
    "        y_train_mean = ([\"tg\"],[y_train_mean]),\n",
    "        y_train_sd = ([\"tg\"],[y_train_sd])\n",
    "    ),\n",
    "    coords=dict(\n",
    "        time=predictors.time.isel(time=np.arange(n_steps-1,len(predictors.time))),\n",
    "        lon=lstm.lon,\n",
    "        lat=lstm.lat,\n",
    "        i_lstm = ([\"tg\"],[idxmin['i'].values]),\n",
    "        it_lstm = ([\"tg\"],[idxmin['it'].values]),\n",
    "    ),\n",
    "    attrs=dict(description=\"LSTM with lowest RMSE above the observed 99th percentile applied to HighResMIP data\",model=highresmip_model),)\n",
    "        \n",
    "                                      )\n",
    "    \n",
    "surge_ds = xr.merge(surge_output)\n",
    "surge_ds.to_netcdf(os.path.join('/home/jovyan/test_surge_models/results/nn_tests/highresmip_predictions',\n",
    "                                highresmip_model+'_lstm_minRMSE_predictions'+['','_alpha0'][only_alpha0]+['','_standardized_x_with_era5'][standardize_predictors_with_era5]+'.nc'),mode='w')\n",
    "#surge_ds.to_netcdf(os.path.join(output_dir,'lstm_surge_'+highresmip_model)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77aa090-15b9-4e0f-a864-f2535411ee27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
