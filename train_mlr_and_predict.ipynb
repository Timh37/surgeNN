{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "680c232e-f2a9-4751-9416-6eba6adee8d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-01 13:04:14.768938: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-01 13:04:14.843272: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import fnmatch\n",
    "import numpy as np\n",
    "import os\n",
    "import keras\n",
    "from surgeNN.io import load_predictand,load_predictors\n",
    "from surgeNN.preprocessing import split_predictand_and_predictors_chronological,split_predictand_and_predictors_stratified_years\n",
    "from surgeNN.preprocessing import generate_batched_windowed_filtered_tf_input, generate_windowed_filtered_np_input, deseasonalize_da, deseasonalize_df_var\n",
    "from surgeNN.evaluation import add_error_metrics_to_prediction_ds,rmse\n",
    "from surgeNN.preprocessing import standardize_predictand_splits, standardize_predictor_splits\n",
    "from surgeNN.models import train_gssr_mlr, predict_gssr_mlr\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04039e44-4e82-455e-b417-3dc0dff548b2",
   "metadata": {},
   "source": [
    "Configure the script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4bb50d9-5b70-4507-a1e2-d22b5b9863a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tgs        = ['stavanger-svg-nor-nhs.csv','wick-wic-gbr-bodc.csv','esbjerg-esb-dnk-dmi.csv',\n",
    "                  'immingham-imm-gbr-bodc.csv','den_helder-denhdr-nld-rws.csv', 'fishguard-fis-gbr-bodc.csv',  \n",
    "                  'brest-822a-fra-uhslc.csv', 'vigo-vigo-esp-ieo.csv',  'alicante_i_outer_harbour-alio-esp-da_mm.csv']\n",
    "tgs = ['alicante_i_outer_harbour-alio-esp-da_mm.csv']\n",
    "temp_freq = 3 # [hours] temporal frequency to use\n",
    "n_cells   = 5 #n x n grid cells around tide gauge of predictor data to use\n",
    "\n",
    "this_n_steps = 9\n",
    "\n",
    "var_names = ['msl','u10','v10',\n",
    "            'u10_sqd','v10_sqd',\n",
    "            'u10_cbd','v10_cbd'] #variables to use\n",
    "\n",
    "architecture = 'mlr'\n",
    "\n",
    "input_dir  = '/home/jovyan/test_surge_models/input/' #directory with predictor & predictand data\n",
    "output_dir = os.path.join('/home/jovyan/test_surge_models/results/mlr_4p5x4p5/') #where to store the results\n",
    "\n",
    "split_fractions = [.6,.2,.2] #train, test, val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ce2d23-377b-4e8d-969f-39e6a55326e2",
   "metadata": {},
   "source": [
    "Load in & preprocess data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5a271c9-2f01-4e42-9868-fdc198bfa69b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [18:23<00:00, 1103.85s/it]\n"
     ]
    }
   ],
   "source": [
    "tg_lons = []\n",
    "tg_lats = []\n",
    "\n",
    "for tg in tqdm(tgs): #loop over TGs\n",
    "    #load & process predictors\n",
    "    predictors = load_predictors('gs://leap-persistent/timh37/era5_predictors/'+str(temp_freq)+'hourly',tg,n_cells) #open predictor xarray dataset\n",
    "    predictors = predictors.sel(time=slice('1979','2017')) #2018 because of end year GTSM simulations that are used as benchmark\n",
    "\n",
    "    predictors = predictors.isel(lon_around_tg = np.arange(1,19),lat_around_tg = np.arange(1,19)) #reduce to 4 by 4 degree \n",
    "    for var in var_names: #add higher order predictors\n",
    "        if '_sqd' in var:\n",
    "            predictors[var] = predictors[var.split('_')[0]]**2\n",
    "        elif '_cbd' in var:\n",
    "            predictors[var] = predictors[var.split('_')[0]]**3\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    for var in var_names: #remove amean\n",
    "        predictors[var] = predictors[var].groupby(predictors.time.dt.year) - predictors[var].groupby(predictors.time.dt.year).mean('time')\n",
    "        predictors[var] = deseasonalize_da(predictors[var])\n",
    "    #also remove seasmean??\n",
    "    predictors['stacked'] = predictors[var_names].to_array(dim=\"var\") #put predictor variables into one array\n",
    "    predictors = predictors[['stacked']]\n",
    "    predictors['stacked'] = predictors['stacked'].transpose(\"time\",\"var\",\"lon_around_tg\",...)#.stack(f=['var','lon_around_tg','lat_around_tg'],create_index=False)\n",
    "    \n",
    "    #load & process predictands\n",
    "    predictand = load_predictand('/home/jovyan/test_surge_models/input/t_tide_'+str(temp_freq)+'h_hourly_deseasoned_predictands',tg) #open predictand csv\n",
    "    #predictand = load_predictand('/home/jovyan/test_surge_models/input/t_tide_'+str(temp_freq)+'h_anoms_predictands',tg) #open predictand csv\n",
    "    predictand = predictand[(predictand['date']>=predictors.time.isel(time=0).values) & (predictand['date']<=predictors.time.isel(time=-1).values)]  # only use predictands in the period for which we also have predictor values, including at preceding timesteps\n",
    "    \n",
    "    predictand = deseasonalize_df_var(predictand,'surge','date')\n",
    "    \n",
    "    predictand = predictand.set_index('date').resample(str(temp_freq)+'h').fillna(method=None) #insert nans where timesteps are missing\n",
    "    predictand = predictand.reset_index()[['surge','date','lon','lat']]\n",
    "\n",
    "    #train & evaluate models n_runs times:\n",
    "    tg_datasets = [] #list to store output\n",
    "\n",
    "    #generate splits\n",
    "    #idx_train,idx_val,idx_test,x_train,x_val,x_test,y_train,y_val,y_test = split_predictand_and_predictors_chronological(predictand,predictors,split_fractions,this_n_steps)\n",
    "    idx_train,idx_val,idx_test,x_train,x_val,x_test,y_train,y_val,y_test = split_predictand_and_predictors_stratified_years(predictand,predictors,split_fractions,this_n_steps,7,seed=0,how='99pct')\n",
    "\n",
    "    #standardize the input based on the mean & sd of the train split\n",
    "    y_train,y_val,y_test,y_train_mean,y_train_sd = standardize_predictand_splits(y_train,y_val,y_test,output_transform = True)\n",
    "    x_train,x_val,x_test = standardize_predictor_splits(x_train,x_val,x_test)\n",
    "\n",
    "    #get values & timestamps of observations to compare predictions with\n",
    "    t_train = predictand['date'].values[idx_train][np.isfinite(y_train)]\n",
    "    t_val = predictand['date'].values[idx_val][np.isfinite(y_val)]\n",
    "    t_test = predictand['date'].values[idx_test][np.isfinite(y_test)]\n",
    "\n",
    "    #get windowed predictors & filter rows with nan observations from inputs (for tensorflow pipeline (to handle out-of-memory windowed predictors) see code below)\n",
    "    x_train,y_train = generate_windowed_filtered_np_input(x_train.stacked.values,y_train,this_n_steps)\n",
    "    x_val,y_val = generate_windowed_filtered_np_input(x_val.stacked.values,y_val,this_n_steps)\n",
    "    x_test,y_test = generate_windowed_filtered_np_input(x_test.stacked.values,y_test,this_n_steps)\n",
    "    \n",
    "    x_train,x_val,x_test = [np.reshape(k,(k.shape[0],np.prod(k.shape[1::]))) for k in [x_train,x_val,x_test]]\n",
    "    \n",
    "    o_train,o_val,o_test = [y_train_sd * k + y_train_mean for k in [y_train,y_val,y_test]] #back-transform observations\n",
    "\n",
    "    mlr_coefs,train_components = train_gssr_mlr(x_train,y_train)\n",
    "    \n",
    "    #store model\n",
    "    coef_ds = xr.Dataset(\n",
    "            data_vars=dict(\n",
    "                mlr_coefs=(['tg','coef'],mlr_coefs[np.newaxis,:])\n",
    "                ),\n",
    "            coords=dict(\n",
    "                coef=np.arange(len(mlr_coefs)),\n",
    "                tg=[tg]\n",
    "            ),\n",
    "            )\n",
    "    \n",
    "    my_fn = architecture+'_4p5x4p5_'+str(temp_freq)+'h_'+tg.replace('.csv','')\n",
    "    coef_ds.to_netcdf(os.path.join(output_dir,'mlr_models',my_fn+'_gssr_mlr_coefs.nc'))\n",
    "\n",
    "    #store pc spatial patterns into netcdf\n",
    "    components_ds = xr.Dataset(\n",
    "        data_vars=dict(\n",
    "            component=(['tg','pc','f'],train_components[np.newaxis,:,:])\n",
    "            ),\n",
    "        coords=dict(\n",
    "            tg=[tg],\n",
    "            pc=np.arange(train_components.shape[0]),\n",
    "            f=np.arange(x_train.shape[-1])\n",
    "        ),\n",
    "        )\n",
    "\n",
    "    components_ds.to_netcdf(os.path.join(output_dir,'mlr_models',my_fn+'_gssr_mlr_pca_components.nc'))\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    #to-do: predictions for test component as well\n",
    "    prediction_val,prediction_val_components = predict_gssr_mlr(x_val,mlr_coefs,train_components,len(predictors.lon_around_tg),var_names,this_n_steps)\n",
    "    prediction_test,prediction_test_components = predict_gssr_mlr(x_test,mlr_coefs,train_components,len(predictors.lon_around_tg),var_names,this_n_steps)\n",
    "\n",
    "    #make predictions & back-transform        \n",
    "    yhat_val = prediction_val*y_train_sd + y_train_mean\n",
    "    yhat_test = prediction_test*y_train_sd + y_train_mean\n",
    "\n",
    "    #store into xr dataset\n",
    "    ds_val = xr.Dataset(data_vars=dict(o=([\"time\"], o_val),yhat=([\"time\"], yhat_val)),\n",
    "    coords=dict(time=t_val,),\n",
    "    attrs=dict(description=\"MLR prediction performance.\"),)\n",
    "    \n",
    "    ds_test = xr.Dataset(data_vars=dict(o=([\"time\"], o_test),yhat=([\"time\"], yhat_test)),\n",
    "    coords=dict(time=t_test,),\n",
    "    attrs=dict(description=\"MLR prediction performance.\"),)\n",
    "\n",
    "    ds_i = xr.concat((ds_val,ds_test),dim='split',coords='different') #concatenate results for each split\n",
    "    ds_i = ds_i.assign_coords(split = ['val','test'])\n",
    "    out_ds = ds_i \n",
    "#concatenate across runs & compute statistics\n",
    "out_ds = add_error_metrics_to_prediction_ds(out_ds,[.95,.98,.99,.995])\n",
    "\n",
    "out_ds = out_ds.assign_coords(tg = np.array([tg]))\n",
    "\n",
    "out_ds = out_ds.assign_coords(lon = ('tg',np.array([predictand['lon'].values[0]])))\n",
    "out_ds = out_ds.assign_coords(lat = ('tg',np.array([predictand['lat'].values[0]])))\n",
    "\n",
    "out_ds.attrs['temp_freq'] = temp_freq\n",
    "out_ds.attrs['n_cells'] = n_cells\n",
    "\n",
    "my_path = os.path.join(output_dir,'performance')\n",
    "my_fn = architecture+'_4p5x4p5_'+str(temp_freq)+'h_'+tg.replace('.csv','')\n",
    "\n",
    "out_ds.to_netcdf(os.path.join(my_path,my_fn+'.nc'),mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31eea628-f28c-458b-955f-c045f81b541a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(66485, 25200)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictors.isel(lon_around_tg = np.arange(1,19),lat_around_tg = np.arange(1,19))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd621a56-bde2-459c-b109-078c754ba84d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(66485,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa7d3c3-0c35-4e09-8778-b062baf1c8f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
