{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "680c232e-f2a9-4751-9416-6eba6adee8d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 13:49:28.646155: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-15 13:49:28.714781: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import fnmatch\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import keras\n",
    "from surgeNN.io import load_predictand,load_predictors\n",
    "from surgeNN.denseLoss import get_denseloss_weights #if starting with a clean environment, first, in terminal, do->'mamba install kdepy'\n",
    "from surgeNN.preprocessing import split_predictand_and_predictors_chronological,split_predictand_and_predictors_with_stratified_years\n",
    "from surgeNN.preprocessing import generate_batched_windowed_filtered_tf_input, generate_windowed_filtered_np_input, deseasonalize_da, deseasonalize_df_var\n",
    "from surgeNN.evaluation import add_error_metrics_to_prediction_ds,rmse\n",
    "from surgeNN.preprocessing import stack_predictors_for_lstm, stack_predictors_for_convlstm, normalize_predictand_splits, normalize_predictor_splits,standardize_predictand_splits, standardize_predictor_splits\n",
    "from surgeNN.models import build_LSTM_stacked, build_ConvLSTM2D_with_channels\n",
    "from surgeNN.losses import gevl,exp_negexp_mse,obs_squared_weighted_mse, obs_weighted_mse\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "\n",
    "import gc #callback to clean up garbage after each epoch, not sure if needed (usage: callbacks = [GC_Callback()])\n",
    "class GC_Callback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        gc.collect()\n",
    "        \n",
    "#notebook version of train_and_predict.py, used mainly for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04039e44-4e82-455e-b417-3dc0dff548b2",
   "metadata": {},
   "source": [
    "Configure the script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4bb50d9-5b70-4507-a1e2-d22b5b9863a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tgs        = ['barcelona-bar-esp-cmems.csv','den_helder-denhdr-nld-rws.csv', 'brest-822a-fra-uhslc.csv', \n",
    "            'immingham-imm-gbr-bodc.csv','stavanger-svg-nor-nhs.csv'] #tide gauges to process\n",
    "tgs = ['den_helder-denhdr-nld-rws.csv']\n",
    "temp_freq = 3 # [hours] temporal frequency to use\n",
    "n_degrees = 1.5 #n x n grid cells around tide gauge of predictor data to use\n",
    "\n",
    "var_names = ['msl','u10','v10','w'] #variables to use\n",
    "\n",
    "loss_function = 'mse' # default tensorflow loss function string or string of custom loss function of surgeNN.losses (e.g., 'gevl({gamma})')\n",
    "architecture = 'convlstm'\n",
    "\n",
    "input_dir  = '/home/jovyan/test_surge_models/input/' #directory with predictor & predictand data\n",
    "output_dir = '/home/jovyan/test_surge_models/results/nn_tests/'#performance/',architecture+'/') #where to store the results\n",
    "\n",
    "n_runs = 24 #how many iterations with different hyperparameter combinations to run\n",
    "n_iterations = 1\n",
    "n_epochs = 100 #how many training epochs\n",
    "patience = 10 #early stopping patience\n",
    "\n",
    "store_model = 0\n",
    "\n",
    "split_fractions = [.6,.2,.2] #train, test, val\n",
    "split_method = '99pct'\n",
    "split_start_month = 7\n",
    "split_seed = 0\n",
    "\n",
    "#hyperparameters:\n",
    "dl_alpha = np.array([0,1,3,5]).astype('int')\n",
    "batch_size = np.array([128]).astype('int')\n",
    "n_steps = np.array([9]).astype('int') #at and 12h or 24h or 36h before (to-do: also use timesteps after?)\n",
    "n_convlstm = np.array([1]).astype('int')\n",
    "n_convlstm_units = np.array([32]).astype('int')\n",
    "n_dense = np.array([2]).astype('int')\n",
    "n_dense_units = np.array([32]).astype('int')\n",
    "dropout = np.array([0.1,0.2])\n",
    "lrs = np.array([1e-5,5e-5,1e-4])\n",
    "l1s = np.array([0.02])\n",
    "\n",
    "possible_params = [batch_size, n_steps, n_convlstm, n_convlstm_units,\n",
    "                    n_dense, n_dense_units, dropout, lrs, l1s, dl_alpha]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "226ba69b-ba6c-42b3-a998-9f68a46124f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['leap-persistent/timh37/era5_predictors/3hourly/alicante_i_outer_harbour-alio-esp-da_mm_era5Predictors_5x5.nc',\n",
       " 'leap-persistent/timh37/era5_predictors/3hourly/brest-822a-fra-uhslc_era5Predictors_5x5.nc',\n",
       " 'leap-persistent/timh37/era5_predictors/3hourly/den_helder-denhdr-nld-rws_era5Predictors_5x5.nc',\n",
       " 'leap-persistent/timh37/era5_predictors/3hourly/esbjerg-esb-dnk-dmi_era5Predictors_5x5.nc',\n",
       " 'leap-persistent/timh37/era5_predictors/3hourly/fishguard-fis-gbr-bodc_era5Predictors_5x5.nc',\n",
       " 'leap-persistent/timh37/era5_predictors/3hourly/immingham-imm-gbr-bodc_era5Predictors_5x5.nc',\n",
       " 'leap-persistent/timh37/era5_predictors/3hourly/stavanger-svg-nor-nhs_era5Predictors_5x5.nc',\n",
       " 'leap-persistent/timh37/era5_predictors/3hourly/vigo-vigo-esp-ieo_era5Predictors_5x5.nc',\n",
       " 'leap-persistent/timh37/era5_predictors/3hourly/wick-wic-gbr-bodc_era5Predictors_5x5.nc']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gcsfs\n",
    "fs = gcsfs.GCSFileSystem() #list stores, stripp zarr from filename, load \n",
    "fs.ls('gs://leap-persistent/timh37/era5_predictors/3hourly')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ce2d23-377b-4e8d-969f-39e6a55326e2",
   "metadata": {},
   "source": [
    "Load in & preprocess data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f5a271c9-2f01-4e42-9868-fdc198bfa69b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "520/520 - 24s - loss: 1.9384 - accuracy: 0.0000e+00 - val_loss: 0.9640 - val_accuracy: 0.0000e+00 - 24s/epoch - 47ms/step\n",
      "Epoch 2/100\n",
      "520/520 - 23s - loss: 0.9159 - accuracy: 0.0000e+00 - val_loss: 0.6819 - val_accuracy: 0.0000e+00 - 23s/epoch - 43ms/step\n",
      "Epoch 3/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:56<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 111\u001b[0m\n\u001b[1;32m    108\u001b[0m     train_history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(x\u001b[38;5;241m=\u001b[39mx_train,y\u001b[38;5;241m=\u001b[39my_train,epochs\u001b[38;5;241m=\u001b[39mn_epochs,batch_size\u001b[38;5;241m=\u001b[39mthis_batch_size,sample_weight\u001b[38;5;241m=\u001b[39mw_train,validation_data\u001b[38;5;241m=\u001b[39m(x_val,y_val,w_val),callbacks\u001b[38;5;241m=\u001b[39m[tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mEarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39mpatience,\n\u001b[1;32m    109\u001b[0m                         restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),GC_Callback()],verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;66;03m#with numpy arrays input\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: \u001b[38;5;66;03m#else\u001b[39;00m\n\u001b[0;32m--> 111\u001b[0m     train_history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthis_batch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEarlyStopping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmonitor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval_loss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpatience\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mrestore_best_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mGC_Callback\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#with numpy arrays input\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m#make predictions & back-transform\u001b[39;00m\n\u001b[1;32m    115\u001b[0m yhat_train \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(x_train,verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mflatten()\u001b[38;5;241m*\u001b[39my_train_sd \u001b[38;5;241m+\u001b[39m y_train_mean\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/keras/engine/training.py:1650\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1642\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1643\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1644\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1648\u001b[0m ):\n\u001b[1;32m   1649\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1650\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1651\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1652\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:880\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    877\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    879\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 880\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    882\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    883\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:912\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    909\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    910\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    911\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 912\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    914\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    915\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    916\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:134\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    132\u001b[0m   (concrete_function,\n\u001b[1;32m    133\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 134\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1745\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1741\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1743\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1744\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1745\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1746\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1747\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m     args,\n\u001b[1;32m   1749\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1750\u001b[0m     executing_eagerly)\n\u001b[1;32m   1751\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:378\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    377\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 378\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    385\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    386\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    387\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    390\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    391\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tg_lons = []\n",
    "tg_lats = []\n",
    "\n",
    "try:\n",
    "    loss_function_ = eval(loss_function)\n",
    "except:\n",
    "    loss_function_ = loss_function\n",
    "    \n",
    "for tg in tqdm(tgs): #loop over TGs\n",
    "    #load & process predictors\n",
    "    #predictors = load_predictors('/home/jovyan/test_surge_models/input/predictors_'+str(temp_freq)+'hourly',tg,n_cells) #open predictor xarray data\n",
    "    predictors = load_predictors('gs://leap-persistent/timh37/era5_predictors/'+str(temp_freq)+'hourly',tg) #open predictor xarray data\n",
    "    predictors = predictors.sel(time=slice('1979','2017')) #2018 because of end year GTSM simulations that are used as benchmark\n",
    "\n",
    "    if n_degrees > 5:\n",
    "        print('Cannot use more grid cells than provided in input files, so setting n_degrees to 5 instead of '+str(n_degrees))\n",
    "        n_degrees = 5\n",
    "    n_cells = int(n_degrees * 4) #era5 resolution = 0.25 degree\n",
    "    \n",
    "    predictors = predictors.isel(lon_around_tg = np.arange(0+int((20-n_cells)/2),20-int((20-n_cells)/2)),\n",
    "                                 lat_around_tg = np.arange(0+int((20-n_cells)/2),20-int((20-n_cells)/2))) #standard is 20 by 20, reduce if n_cells<5\n",
    "    \n",
    "    if 'w' in var_names and 'w' not in predictors.variables:\n",
    "        predictors['w'] == np.sqrt((predictors.u10**2+predictors.v10**2))\n",
    "        \n",
    "    for var in var_names: #remove amean\n",
    "        predictors[var] = predictors[var].groupby(predictors.time.dt.year) - predictors[var].groupby(predictors.time.dt.year).mean('time') #remove annual means\n",
    "        predictors[var] = deseasonalize_da(predictors[var]) #remove mean seasonal cycle\n",
    "\n",
    "    #load & process predictands\n",
    "    \n",
    "    #----test bandstop filter----\n",
    "    #predictand = load_predictand('/home/jovyan/test_surge_models/input/t_tide_'+str(1)+'h_anoms_deseasoned_predictands',tg) #open predictand csv\n",
    "    predictand = load_predictand('/home/jovyan/test_surge_models/input/t_tide_'+str(temp_freq)+'h_hourly_deseasoned_predictands',tg) #open predictand csv\n",
    "    \n",
    "    predictand = predictand[(predictand['date']>=predictors.time.isel(time=0).values) & (predictand['date']<=predictors.time.isel(time=-1).values)]  # only use predictands in the period for which we also have predictor values, including at preceding timesteps\n",
    "    predictand = deseasonalize_df_var(predictand,'surge','date') #remove mean seasonal cycle\n",
    "    \n",
    "    #fill missing values before computing filter\n",
    "    #predictand = bandstop_filter_hourly_predictand(predictand,30,265,345,10)\n",
    "    #----test bandstop filter----\n",
    "    \n",
    "    predictand = predictand.set_index('date').resample(str(temp_freq)+'h').fillna(method=None) #insert nans where timesteps are missing\n",
    "    predictand = predictand.reset_index()[['surge','date','lon','lat']]\n",
    "    #predictand['surge'] = predictand['surge'].rolling(window=int(12/temp_freq+1),min_periods=int(12/temp_freq+1),center=True).mean() #crude way to filter out peaks due to uncorrected tides (Tiggeloven et al., 2021)\n",
    "  \n",
    "    #configure hyperparameter settings\n",
    "    possible_settings = list(itertools.product(*possible_params))\n",
    "    n_settings = len(possible_settings)\n",
    "\n",
    "    if n_runs<n_settings:\n",
    "        selected_settings = np.random.choice(possible_settings, n_runs, replace=False)\n",
    "    else:\n",
    "        selected_settings = possible_settings\n",
    "            \n",
    "    #train & evaluate models n_iterations * n_runs times:\n",
    "    for it in np.arange(n_iterations):\n",
    "        tg_datasets = [] #list to store output\n",
    "        \n",
    "        for i,these_settings in enumerate(selected_settings): #loop over hyperparameter combinations\n",
    "            this_batch_size,this_n_steps,this_n_convlstm,this_n_convlstm_units,this_n_dense,this_n_dense_units,this_dropout,this_lr,this_l2,this_dl_alpha = these_settings\n",
    "                \n",
    "            #generate splits\n",
    "            #idx_train,idx_val,idx_test,x_train,x_val,x_test,y_train,y_val,y_test = split_predictand_and_predictors_chronological(predictand,predictors,split_fractions,this_n_steps)\n",
    "            idx_train,idx_val,idx_test,x_train,x_val,x_test,y_train,y_val,y_test = split_predictand_and_predictors_with_stratified_years(predictand,predictors,\n",
    "                                                                                                                                    split_fractions,this_n_steps,split_start_month,seed=split_seed,how=split_method)\n",
    "\n",
    "            #standardize the input based on the mean & sd of the train split\n",
    "            y_train,y_val,y_test,y_train_mean,y_train_sd = standardize_predictand_splits(y_train,y_val,y_test,output_transform = True)\n",
    "            x_train,x_val,x_test = standardize_predictor_splits(x_train,x_val,x_test)\n",
    "\n",
    "            if architecture == 'convlstm':\n",
    "                x_train, x_val, x_test = [stack_predictors_for_convlstm(k,var_names) for k in [x_train,x_val,x_test]] #stack all predictor variables at each grid cell for LSTM input\n",
    "            elif architecture == 'lstm':\n",
    "                x_train, x_val, x_test = [stack_predictors_for_lstm(k,var_names) for k in [x_train,x_val,x_test]] #stack all predictor variables at each grid cell for LSTM input\n",
    "\n",
    "            #get Denseloss weights\n",
    "            w_train,w_val = [get_denseloss_weights(k, alpha = this_dl_alpha ) for k in [y_train,y_val]] #generate DenseLoss weights for train & val splits   \n",
    "\n",
    "            #get values & timestamps of observations to compare predictions with\n",
    "            t_train = predictand['date'].values[idx_train][np.isfinite(y_train)]\n",
    "            t_val = predictand['date'].values[idx_val][np.isfinite(y_val)]\n",
    "            t_test = predictand['date'].values[idx_test][np.isfinite(y_test)]\n",
    "\n",
    "            #get windowed predictors & filter rows with nan observations from inputs (for tensorflow pipeline (to handle out-of-memory windowed predictors) see code below)\n",
    "            x_train,y_train,w_train = generate_windowed_filtered_np_input(x_train,y_train,this_n_steps,w_train)\n",
    "            x_val,y_val,w_val = generate_windowed_filtered_np_input(x_val,y_val,this_n_steps,w_val)\n",
    "            x_test,y_test = generate_windowed_filtered_np_input(x_test,y_test,this_n_steps)\n",
    "\n",
    "            o_train,o_val,o_test = [y_train_sd * k + y_train_mean for k in [y_train,y_val,y_test]] #back-transform observations\n",
    "\n",
    "            #build model\n",
    "            if architecture == 'convlstm':\n",
    "                model = build_ConvLSTM2D_with_channels(this_n_convlstm, this_n_dense,\n",
    "                                                       (np.ones(this_n_convlstm)*this_n_convlstm_units).astype(int), \n",
    "                                          (np.ones(this_n_dense)*this_n_dense_units).astype(int),\n",
    "                                          this_n_steps,n_cells,n_cells,len(var_names), 'convlstm0',\n",
    "                                                       this_dropout, this_lr, loss_function_,l2=this_l2)\n",
    "            elif architecture == 'lstm':\n",
    "                model = build_LSTM_stacked(this_n_convlstm, this_n_dense, \n",
    "                                  (np.ones(this_n_convlstm)*this_n_convlstm_units).astype(int), \n",
    "                                  (np.ones(this_n_dense)*this_n_dense_units).astype(int), \n",
    "                                   this_n_steps,n_cells,n_cells,len(var_names), 'lstm0',\n",
    "                                   this_dropout, this_lr, loss_function_,l2=this_l2) #loss_function\n",
    "\n",
    "            #train model:\n",
    "            if this_dl_alpha: #if using DenseLoss weights\n",
    "                train_history = model.fit(x=x_train,y=y_train,epochs=n_epochs,batch_size=this_batch_size,sample_weight=w_train,validation_data=(x_val,y_val,w_val),callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience,\n",
    "                                    restore_best_weights=True),GC_Callback()],verbose=2) #with numpy arrays input\n",
    "            else: #else\n",
    "                train_history = model.fit(x=x_train,y=y_train,epochs=n_epochs,batch_size=this_batch_size,validation_data=(x_val,y_val),callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience,\n",
    "                                    restore_best_weights=True),GC_Callback()],verbose=2) #with numpy arrays input\n",
    "\n",
    "            #make predictions & back-transform\n",
    "            yhat_train = model.predict(x_train,verbose=0).flatten()*y_train_sd + y_train_mean\n",
    "            yhat_val = model.predict(x_val,verbose=0).flatten()*y_train_sd + y_train_mean\n",
    "            yhat_test = model.predict(x_test,verbose=0).flatten()*y_train_sd + y_train_mean\n",
    "\n",
    "            #store results into xr dataset for current i-j\n",
    "            ds_train = xr.Dataset(data_vars=dict(o=([\"time\"], o_train),yhat=([\"time\"], yhat_train),hyperparameters=(['p'],these_params),),\n",
    "            coords=dict(time=t_train,p=['batch_size', 'n_steps', 'n_convlstm', 'n_convlstm_units','n_dense', 'n_dense_units', 'dropout', 'lr', 'l2','dl_alpha'],),\n",
    "            attrs=dict(description=\"ConvLSTM - neural network prediction performance.\",loss_function=loss_function),)\n",
    "\n",
    "            ds_val = xr.Dataset(data_vars=dict(o=([\"time\"], o_val),yhat=([\"time\"], yhat_val),hyperparameters=(['p'],these_params),),\n",
    "            coords=dict(time=t_val,p=['batch_size', 'n_steps', 'n_convlstm', 'n_convlstm_units','n_dense', 'n_dense_units', 'dropout', 'lr', 'l2','dl_alpha'],),\n",
    "            attrs=dict(description=\"ConvLSTM - neural network prediction performance.\",loss_function=loss_function),)\n",
    "\n",
    "            ds_test = xr.Dataset(data_vars=dict(o=([\"time\"], o_test),yhat=([\"time\"], yhat_test),hyperparameters=(['p'],these_params),),\n",
    "            coords=dict(time=t_test,p=['batch_size', 'n_steps', 'n_convlstm', 'n_convlstm_units','n_dense', 'n_dense_units', 'dropout', 'lr', 'l2','dl_alpha'],),\n",
    "            attrs=dict(description=\"ConvLSTM - neural network prediction performance.\",loss_function=loss_function),)\n",
    "\n",
    "            ds_i = xr.concat((ds_train,ds_val,ds_test),dim='split',coords='different') #concatenate results for each split\n",
    "            ds_i = ds_i.assign_coords(split = ['train','val','test'])\n",
    "\n",
    "            loss = np.nan*np.zeros(n_epochs) #add loss of training\n",
    "            val_loss = np.nan*np.zeros(n_epochs)\n",
    "\n",
    "            loss[0:len(train_history.history['loss'])] = train_history.history['loss']\n",
    "            val_loss[0:len(train_history.history['val_loss'])] = train_history.history['val_loss']\n",
    "\n",
    "            ds_i['loss'] = (['e'],loss)\n",
    "            ds_i['val_loss'] = (['e'],val_loss)\n",
    "\n",
    "            tg_datasets.append(ds_i) #append output of current iteration to list of all outputs\n",
    "            \n",
    "            if store_model:\n",
    "                my_path = os.path.join(output_dir,'keras_models',architecture)\n",
    "                my_fn = architecture+'_'+str(temp_freq)+'h_'+tg.replace('.csv','')+'_'+loss_function+'_hp1_i'+str(i)+'_it'\n",
    "                \n",
    "                model.save(os.path.join(my_path,\n",
    "                 my_fn+str(len(fnmatch.filter(os.listdir(my_path),my_fn+'*')))+'.keras'))\n",
    "            \n",
    "            del model, train_history, ds_i #, x_train, x_val, x_test\n",
    "            tf.keras.backend.clear_session()\n",
    "            gc.collect()\n",
    "\n",
    "        #concatenate across runs & compute statistics\n",
    "        out_ds = xr.concat(tg_datasets,dim='i',coords='different')\n",
    "        out_ds = add_error_metrics_to_prediction_ds(out_ds,[.95,.98,.99,.995])\n",
    "\n",
    "        out_ds = out_ds.assign_coords(tg = np.array([tg]))\n",
    "        out_ds = out_ds.assign_coords(lon = ('tg',np.array([predictand['lon'].values[0]])))\n",
    "        out_ds = out_ds.assign_coords(lat = ('tg',np.array([predictand['lat'].values[0]])))\n",
    "\n",
    "        out_ds.attrs['temp_freq'] = temp_freq\n",
    "        out_ds.attrs['n_cells'] = n_cells\n",
    "        out_ds.attrs['n_epochs'] = n_epochs\n",
    "        out_ds.attrs['patience'] = patience\n",
    "        out_ds.attrs['loss_function'] = loss_function\n",
    "        out_ds.attrs['split_fractions'] = split_fractions\n",
    "        out_ds.attrs['split_method'] = split_method+'_'+str(split_start_month)+'_'+str(split_seed)\n",
    "\n",
    "        my_path = os.path.join(output_dir,'performance',architecture)\n",
    "        my_fn = architecture+'_'+str(temp_freq)+'h_'+tg.replace('.csv','')+'_'+loss_function+'_hp1_ndeg'+str(n_degrees)+'_it'\n",
    "        #out_ds.to_netcdf(os.path.join(output_dir,out_fn+str(len(fnmatch.filter(os.listdir(output_dir),out_fn+'*')))+'bsfilter.nc'),mode='w')\n",
    "\n",
    "        #out_ds.to_netcdf(os.path.join(output_dir,architecture+'_'+str(temp_freq)+'h_lossfunc_test_'+tg.replace('.csv','')+'_'+loss_function.replace('DenseLoss','DenseLoss'+str(alpha)).replace('gevl','gevl'+str(gamma)).replace('mse_exp','mse_exp'+str(a)+'_'+str(t))+'10it.nc'),mode='w')\n",
    "        #out_ds.to_netcdf(os.path.join(output_dir,'lstm_lossfunc_test_'+tg.replace('.csv','')+'_'+loss_function.replace('DenseLoss','DenseLoss'+str(alpha)).replace('gevl','gevl'+str(gamma))+'.nc'),mode='w')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39497191-1001-4f08-a07f-95c5e1c5c88e",
   "metadata": {
    "tags": []
   },
   "source": [
    "```python\n",
    "threshold_pct=.99\n",
    "threshold_value = out_ds.o.isel(i=0).sel(split='test').quantile(threshold_pct,dim='time').values\n",
    "\n",
    "y_true = out_ds.o.isel(i=0).sel(split='test')#.resample(time='1D')#.dropna(dim='time')\n",
    "\n",
    "surge_codec = xr.open_dataset('/home/jovyan/test_surge_models/CoDEC_ERA5_at_gesla3_tgs_eu_hourly_anoms.nc')\n",
    "surge_codec['surge'] = deseasonalize_da(surge_codec['surge'])\n",
    "surge_codec = surge_codec.sel(tg=tg).sel(time=out_ds.time.where(np.isfinite(y_true),drop=True))\n",
    "\n",
    "#surge_codec['surge'] = surge_codec['surge'].rolling(time=int(12/temp_freq+1),min_periods=1,center=True).mean()\n",
    "surge_codec = surge_codec#.resample(time='1D').max(skipna=True)\n",
    "#surge_codec['surge'] = (surge_codec['surge'] - surge_codec['surge'].mean(dim='time'))/surge_codec['surge'].std(dim='time',ddof=0) #normalize\n",
    "#surge_codec_test=surge_codec.surge.value s[np.isfinite(y_true)]\n",
    "#y_true = y_true[np.isfinite(y_true)]\n",
    "#y_true = out_ds.o.isel(i=0).sel(split='test')#.resample(time='1D').max(skipna=True)\n",
    "y_true = y_true.values[np.isfinite(y_true)]\n",
    "#surge_codec_test = surge_codec.sel(tg=tg).sel(time=predictand['date'].values[idx_test][n_steps-1::]).surge.values #select test timesteps\n",
    "surge_codec_test_exceedances = (surge_codec.surge.values>=threshold_value).flatten() #find where exceeding threshold\n",
    "y_true_exceedances = (y_true>=threshold_value)\n",
    "print('---CoDEC---')\n",
    "print('bulk correlation r='+str(np.corrcoef(surge_codec.surge.values,y_true)[0][1]))\n",
    "print('bulk RMSE='+str(rmse(surge_codec.surge.values,y_true)))\n",
    "print('Confusion matrix exceedances above {0}th percentile:'.format(threshold_pct))\n",
    "print(confusion_matrix(y_true_exceedances,surge_codec_test_exceedances))\n",
    "\n",
    "print('Correlation at timesteps where observations above {0}th percentile:'.format(threshold_pct))\n",
    "print('r=' + str(np.corrcoef(surge_codec.surge.values[y_true_exceedances],y_true[y_true_exceedances])[0][1]))\n",
    "print('RMSE at timesteps where observations above {0}th percentile:'.format(threshold_pct))\n",
    "print('RMSE=' + str(rmse(surge_codec.surge.values[y_true_exceedances],y_true[y_true_exceedances])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecd6963-0aa8-4456-aadb-18f0704dbabb",
   "metadata": {},
   "source": [
    "Tensorflow pipeline:\n",
    "\n",
    "```python\n",
    "#get values & timestamps of observations to compare predictions with\n",
    "o_val = y_train_sd * y_val[np.isfinite(y_val)][0:int(np.sum(np.isfinite(y_val))/batch_size)] + y_train_mean #back-transform observations val split\n",
    "o_test = y_train_sd * y_test[np.isfinite(y_test)][0:int(np.sum(np.isfinite(y_val))/batch_size)] + y_train_mean #back-transform observations val split\n",
    "\n",
    "t_val = predictand['date'].values[idx_val][np.isfinite(y_val)][0:int(np.sum(np.isfinite(y_val))/batch_size)]\n",
    "t_test = predictand['date'].values[idx_test][np.isfinite(y_test)][0:int(np.sum(np.isfinite(y_val))/batch_size)]\n",
    "\n",
    "#create windowed predictors, filter out timesteps with NaN observations & create batches:\n",
    "if use_dl == False: #if not using weights\n",
    "    z_train = create_batched_sequenced_datasets(x_train, y_train, this_n_steps, this_batch_size).cache() #cache() speeds up the training by loading in the data at epoch 0, but takes up a lot of memory\n",
    "    z_val = create_batched_sequenced_datasets(x_val, y_val, this_n_steps, this_batch_size).cache()\n",
    "\n",
    "    x_val_ds = z_val.map(lambda a, b : a) #unpack z_val for prediction\n",
    "\n",
    "elif use_dl == True: #if using weights\n",
    "    z_train = create_batched_sequenced_datasets(x_train, y_train, this_n_steps, this_batch_size, w_train).cache()\n",
    "    z_val = create_batched_sequenced_datasets(x_val, y_val, this_n_steps, this_batch_size, w_val).cache()\n",
    "\n",
    "    x_val_ds = z_val.map(lambda a, b, c: a) #unpack z_val for prediction\n",
    "\n",
    "z_test = create_batched_sequenced_datasets(x_test, y_test, this_n_steps, this_batch_size) #to-do: z_test doesn't have to be batched?\n",
    "x_test_ds = z_test.map(lambda a, b: a) #unpack z_test for prediction\n",
    "\n",
    "history = model.fit(z_train,epochs=n_epochs,validation_data=z_val,callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5,\n",
    "                            restore_best_weights=True)],verbose=0) #train model\n",
    "            \n",
    "#make predictions & back-transform\n",
    "yhat_val = model.predict(x_val_ds,verbose=0).flatten()*y_train_sd + y_train_mean\n",
    "yhat_test = model.predict(x_test_ds,verbose=0).flatten()*y_train_sd + y_train_mean\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7bef61-17bf-440c-a64e-c0093a7513bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
